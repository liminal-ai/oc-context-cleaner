# Handoff: oc-context-cleaner Execution

**Date:** 2026-02-01
**For:** Opus 4.5 Orchestrator
**From:** Previous orchestrator session (context exhausted)

---

## Product Summary

**oc-context-cleaner** is a CLI tool for cleaning OpenClaw/Clawdbot session transcripts. It strips or truncates tool calls from session files to reduce context size while preserving conversation content.

**Primary use cases:**
1. **Edit in place** — Agent edits its own session to reduce context (`occ edit --strip-tools`)
2. **Clone with stripping** — Create a new session from existing one with tools removed (`occ clone --strip-tools`)

**Key features:**
- Preset-based tool removal (default, aggressive, extreme)
- Atomic writes with backup rotation (max 5 backups)
- Session discovery (full ID, partial ID, auto-detect current)
- Human and JSON output formats

---

## Project Documents

### Feature Specification
**Path:** `/Users/leemoore/code/agent-cli-tools/oc-context-cleaner/docs/feature-spec.md`

Complete requirements document with:
- User Profile (who uses this, when, why)
- User Flows (edit, clone, list, info, restore)
- Acceptance Criteria (AC-1.x through AC-7.x)
- Test Conditions (TC-x.xa for each AC)
- Data Contracts (session format, config schema)
- Scope boundaries (what's in/out)

### Tech Design
**Path:** `/Users/leemoore/code/agent-cli-tools/oc-context-cleaner/docs/tech-design.md`

Implementation architecture with:
- High/Mid/Low altitude design
- Sequence diagrams for all flows
- Module responsibility matrix
- Complete interface definitions (copy-paste ready)
- TC-to-test mapping
- Work breakdown into chunks/stories

### Stories
**Path:** `/Users/leemoore/code/agent-cli-tools/oc-context-cleaner/docs/stories/`

```
story-0-infrastructure/   — Types, fixtures, error classes
story-1-core-algorithm/   — Turn boundaries, tool removal
story-2-io-layer/         — Session read/write, discovery
story-3-edit-flow/        — Edit command end-to-end
story-4-clone-flow/       — Clone command end-to-end
story-5-support-commands/ — List, info, restore
story-6-configuration/    — Config loading, CLI entry
```

Each story folder contains:
- `story.md` — Overview, ACs covered, files, test counts
- `prompt-X.1-skeleton-red.md` — Skeleton + tests (combined)
- `prompt-X.2-green.md` — Implementation
- `prompt-X.R-verify.md` — Verification checklist

---

## Your Role

You are the **Orchestrator** for Phase 5 (Execution) of this SDD project.

**You do:**
- Coordinate agent execution of stories
- Launch validators (dual-validator pattern: SE + GPT-5.2 Codex)
- Consolidate validation findings (blockers/issues/nits)
- Direct fixes and re-validation
- Verify validator claims against actual code
- Make decisions on ambiguous issues
- Track test counts and story completion

**You don't:**
- Implement stories yourself (launch Senior Engineer subagents)
- Skip validation steps
- Trust validators blindly (spot-check claims)

---

## Load SDD Skill

Load the SDD skill for methodology context:
```
/sdd
```

Then load these specific references for Phase 5 execution:

1. `references/senior-engineer.md` — Engineer role and constraints
2. `references/phase-execution.md` — Story execution cycle details
3. `references/prompting-gpt-5.2.md` — Verification and Codex CLI usage

**IMPORTANT:** Also load this file which is NOT in the released skill yet:
```
/Users/leemoore/code/agent-cli-tools/sdd-skill/references/execution-orchestration.md
```

This documents the orchestration workflow we've been using:
- Dual-validator pattern (Senior Engineer + GPT-5.2 Codex)
- Validation → Fix → Execute → Verify pipeline
- Parallel story processing
- Session management (when to resume vs fresh)
- Human decision points
- Agent selection guide

---

## Current Project Status

### Completed Stories

| Story | Status | Tests |
|-------|--------|-------|
| Story 0: Infrastructure | Complete + Verified | 0 (types only) |
| Story 1: Core Algorithm | Complete + Verified | 12 |
| Story 2: IO Layer | Complete + Verified | 0 (tested via commands) |
| Story 3: Edit Flow | Complete + Verified + Polish done | 19 |

**Running test total:** 31 tests passing

**Quality gates:** All passing (typecheck, lint, tests)

### Current Codebase Structure

```
src/
├── commands/
│   └── edit-command.ts          # Story 3
├── config/
│   └── tool-removal-presets.ts  # Story 3
├── core/
│   ├── backup-manager.ts        # Story 3
│   ├── edit-operation-executor.ts  # Story 3
│   ├── session-parser.ts        # Story 1
│   ├── tool-call-remover.ts     # Story 1
│   └── turn-boundary-calculator.ts  # Story 1
├── io/
│   ├── paths.ts                 # Story 2
│   ├── session-discovery.ts     # Story 2
│   ├── session-file-reader.ts   # Story 2
│   ├── session-file-writer.ts   # Story 2
│   ├── session-index-reader.ts  # Story 2
│   └── session-index-writer.ts  # Story 2
├── output/
│   └── result-formatter.ts      # Story 3
├── types/
│   ├── configuration-types.ts   # Story 0
│   ├── index.ts                 # Story 0
│   ├── operation-types.ts       # Story 0
│   ├── session-types.ts         # Story 0
│   └── tool-removal-types.ts    # Story 0
└── errors.ts                    # Story 0

tests/
├── algorithms/
│   └── tool-call-remover.test.ts  # Story 1 (12 tests)
├── commands/
│   └── edit-command.test.ts       # Story 3 (19 tests)
└── fixtures/
    └── sessions.ts                # Story 0
```

Story 4 will add: `src/core/clone-operation-executor.ts`, `src/commands/clone-command.ts`, `tests/commands/clone-command.test.ts`

### Story 3 Polish (Completed)

During Story 3 verification, GPT-5.2 Codex found polish issues that were fixed:
- Added `✓` checkmark to human output
- Changed `->` to `→` arrow
- Fixed arrow spacing (`→ Y` not `→Y`)
- Added actionable error hints for 6 error types

### Ready for Execution

| Story | Status |
|-------|--------|
| Story 4: Clone Flow | **Validated, ready to execute** |
| Story 5: Support Commands | Not yet validated |
| Story 6: Configuration | Not yet validated |

---

## Story 4 Details

**Story 4: Clone Flow** creates new sessions from existing ones with optional tool stripping.

**Files to create:**
- `src/core/clone-operation-executor.ts`
- `src/commands/clone-command.ts`
- `tests/commands/clone-command.test.ts`

**Files to modify:**
- `src/types/operation-types.ts` — Add `agentId` to `CloneOptions`
- `src/types/session-types.ts` — Add `ClonedSessionHeader` interface

**Tests:** 11 new tests
**Expected running total after Story 4:** 42 tests

**Prompts:**
- `docs/stories/story-4-clone-flow/prompt-4.1-skeleton-red.md`
- `docs/stories/story-4-clone-flow/prompt-4.2-green.md`
- `docs/stories/story-4-clone-flow/prompt-4.R-verify.md`

**Validation status:** PASS (both SE and Codex validated after fixes)

### Story 4 Validation History

Story 4 went through multiple validation rounds. Key issues found and fixed:

| Issue | Found By | Fix Applied |
|-------|----------|-------------|
| `CloneOptions` missing `agentId` field | Both | Added to tech-design + types |
| Test count said 38, should be 42 | Codex | Updated story.md and prompts |
| TC-4.4a test used try/catch (would PASS in Red phase) | Codex | Changed to assert on `result.success` |
| Files section missing `session-types.ts` | Codex | Added to story.md |
| Verify prompt missing lint gate | Both | Added `npm run lint` |
| Types not inlined in prompt 4.1 | SE | Added CloneOptions/CloneResult/CloneStatistics |
| Custom output path registration unclear | Codex | Clarified in story.md |

**Pattern:** Codex caught more pedantic spec-alignment issues. SE caught structural/missing content issues. Using both was valuable.

---

## What's Next

### Immediate: Execute Story 4

**First:** Read the prompts before launching the Senior Engineer:
- `docs/stories/story-4-clone-flow/prompt-4.1-skeleton-red.md`
- `docs/stories/story-4-clone-flow/prompt-4.2-green.md`

This lets you understand what SE will do and catch any issues early.

**Then:**

1. **Launch Senior Engineer** to implement Story 4
   - Execute prompt-4.1-skeleton-red.md (creates stubs + tests)
   - Execute prompt-4.2-green.md (implements to pass tests)
   - Run quality gates: `npm run typecheck && npm run lint && npm test`
   - Expected: 42 tests pass

2. **Have same SE session self-review** the implementation

3. **Launch GPT-5.2 Codex** with prompt-4.R-verify.md
   - Use `workspace-write` sandbox (needs to run tests)
   - Verify TC-by-TC coverage

### Parallel: Validate Story 5

While Story 4 executes, start validating Story 5:

1. **Launch Senior Engineer** to validate Story 5
   - Read story.md, all prompts, tech-design, feature-spec
   - Check structure, self-containment, tech-design alignment, TDD flow
   - Report: blockers/issues/nits

2. **Launch GPT-5.2 Codex** to validate Story 5 (same instructions)

3. **Consolidate findings** and fix any blockers before Story 5 execution

### Pipeline View

```
Story 3: ════[Complete]
Story 4: ════[Execute]════[Verify]════
Story 5:      ════[Validate]════[Fix]════[Execute]════
Story 6:              ════[Validate]════
```

---

## Key Patterns

### Dual-Validator Pattern

For each story validation, launch BOTH:
- **Senior Engineer** (Claude) — Builder mindset, structural issues
- **GPT-5.2 Codex** (high reasoning) — Pedantic, spec drift, edge cases

They find different issues. Consolidate into blockers/issues/nits.

### Example Validator Output Format

Validators return structured reports. Here's what to expect:

```
## Story N Validation: [PASS/FAIL]

**Confidence:** [High/Medium/Low] — [reasoning]

### Blockers (must fix)
1. **[Issue name]** — [description]. Location: [file:line]. Fix: [recommendation]

### Issues (should fix)
1. **[Issue name]** — [description]

### Nits (minor polish)
1. [Description]

### Checklist
- [x] Story structure complete
- [x] Prompts self-contained
- [ ] TC-4.4a test assertion incorrect  ← example failure
- [x] Test counts accurate
```

When consolidating two validators:
1. Merge duplicate findings
2. Take the more severe categorization if they disagree
3. Add items one found that the other missed
4. Note disagreements for human decision

### Fix Cycle

```
Validator finds issues
    ↓
Senior Engineer fixes
    ↓
SAME validator session re-validates (has context)
    ↓
Orchestrator verifies claims against actual files
    ↓
Repeat until PASS
```

### Session Management

**Resume same session when:**
- Re-validating after fixes
- Self-reviewing implementation

**Fresh session when:**
- New story
- Different task type

---

## Commands Reference

**Quality gates:**
```bash
cd /Users/leemoore/code/agent-cli-tools/oc-context-cleaner
npm run typecheck
npm run lint
npm test
```

**Codex CLI validation:**
```bash
codex exec -C /Users/leemoore/code/agent-cli-tools/oc-context-cleaner \
  --sandbox workspace-write -m gpt-5.2-codex \
  -c model_reasoning_effort=high "prompt here"
```

**Codex CLI resume:**
```bash
codex exec resume <session-id> "follow-up prompt"
```

---

## User Preferences (Learned from This Session)

The user has specific working preferences discovered during execution:

**Quality bar:**
- Fix ALL issues, not just blockers — "even if it seems small, if it would make the process or code better, let's do it"
- Only skip fixes if they require "significant rework or refactor"
- Velocity of coding is fast, so take every opportunity to improve quality

**Backfill strategy:**
- When prompts evolve beyond what's in tech-design, UPDATE the tech-design
- Keep artifacts in sync — "we continue to use these tech designs as comprehensive successful designs to look at later"

**Context management:**
- User is "stingy with context" — prefer subagents for research tasks
- Don't burn orchestrator context on web searches or exploration
- Launch subagents for anything that could consume significant tokens

**Transparency:**
- Don't hide small issues because they're "not blockers" — report everything
- When deciding not to take validator advice, explain reasoning so validator can push back
- Always verify validator claims against actual files

**Communication style:**
- Keep responses concise and high signal
- Go into detail when situation calls for it
- Form informed opinions before presenting options

---

## Human Decision Points

These require user input, don't automate:
- Blocker triage (real blocker vs pedantic?)
- Spec vs implementation conflicts (which is right?)
- Effort/quality tradeoff (fix everything vs blockers only?)
- Test count discrepancies (which source is authoritative?)
- Gorilla testing (manual ad-hoc testing)
- Final acceptance ("ship it")

### Gorilla Testing for This Project

For CLI Gorilla testing, suggest the user:
1. Run `occ edit` against a real Clawdbot session with tool calls
2. Run `occ clone` to create a new session
3. Verify the output "feels right" — correct format, sensible stats
4. Try edge cases: non-existent session ID, partial ID matching, `--json` output
5. Check that backup files are created correctly

This cannot be automated — it's human judgment on whether the CLI experience is good.

---

## Prior Session State

**All prior validator sessions are closed.** Start fresh for all new validations.

No Codex session IDs to resume. The fix cycles for Story 4 validation are complete — Story 4 is ready for execution with no pending re-validations.

If you need to re-validate Story 4 for any reason, start a fresh Codex session.

---

## Questions?

If unclear on anything, ask the user before proceeding. The methodology is rigorous for a reason — skipping steps creates downstream problems.
